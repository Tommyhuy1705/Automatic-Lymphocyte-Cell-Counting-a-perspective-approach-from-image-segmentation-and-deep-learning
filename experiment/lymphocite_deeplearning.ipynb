{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===================================================================\n",
    "# INSTALLATION AND IMPORTS\n",
    "# ==================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q segmentation-models-pytorch tqdm albumentations ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4efe6032-5bd4-42c2-bf49-55ee977fd825",
    "_uuid": "732aaf94-e878-4d02-bfd7-44117a4e909f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import urllib.request\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5511e70f-ca6a-4004-b4e9-b17f5b57c9a4",
    "_uuid": "cd9842cf-885a-455b-967f-d8804c3b46ab",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## Part 1: U-Net Model Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f3d93cb1-115e-4488-8beb-49986719a2d8",
    "_uuid": "a3c49b08-12da-4c91-8e15-c553f8bee3f3",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1.1: Prepare Dataset and DataLoader for U-Net\n",
    "# ===================================================================\n",
    "class CellSegDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        # Corresponding mask name\n",
    "        mask_path = os.path.join(self.masks_dir, f\"MASK_{os.path.splitext(img_name)[0]}.png\")\n",
    "\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "\n",
    "        # Convert mask to a tensor (C, H, W)\n",
    "        if isinstance(mask, np.ndarray):\n",
    "            mask = torch.from_numpy((mask > 0).astype(\"np.int64\"))\n",
    "        elif isinstance(mask, torch.Tensor):\n",
    "            mask = (mask > 0).long()\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported mask type: {type(mask)}\")\n",
    "        \n",
    "        # Ensure mask has a channel dimension\n",
    "        if mask.dim() == 2:\n",
    "            mask = mask.unsqueeze(0)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Define image augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.RandomResizedCrop(size=(256, 256), scale=(0.9, 1.0), p=0.3),\n",
    "    A.Affine(scale=(0.95, 1.05), translate_percent=(0.02, 0.02), rotate=(-10, 10), p=0.3),\n",
    "    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_test_transform = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Path to the dataset\n",
    "base_dir = \"/kaggle/input/cell-counting-roboflow-segmentation-masks\"\n",
    "\n",
    "train_dataset = CellSegDataset(\n",
    "    images_dir=os.path.join(base_dir, \"train/images\"),\n",
    "    masks_dir=os.path.join(base_dir, \"train/masks_binary\"),\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = CellSegDataset(\n",
    "    images_dir=os.path.join(base_dir, \"valid/images\"),\n",
    "    masks_dir=os.path.join(base_dir, \"valid/masks_binary\"),\n",
    "    transform=val_test_transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b43952fe-474e-4a55-8286-c35baf03fafe",
    "_uuid": "0b1ce297-6f46-48ac-82d8-e8ed1a1beb8c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1.2: Define U-Net Model, Loss, and Training Function\n",
    "# ===================================================================\n",
    "model_unet_train = smp.Unet(\n",
    "    encoder_name=\"resnet152\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    classes=2,\n",
    "    activation=None,\n",
    ")\n",
    "print(f\"U-Net Model parameters: {sum(p.numel() for p in model_unet_train.parameters()):,}\")\n",
    "\n",
    "# --- Define Loss Functions ---\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for segmentation\"\"\"\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        predictions = torch.softmax(predictions, dim=1)\n",
    "        predictions = predictions[:, 1, :, :] # Only get probabilities for the foreground class\n",
    "        targets = targets.float()\n",
    "        \n",
    "        intersection = (predictions * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combination of CrossEntropy and Dice Loss\"\"\"\n",
    "    def __init__(self, weight_ce=0.5, weight_dice=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.weight_ce = weight_ce\n",
    "        self.weight_dice = weight_dice\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # CrossEntropy Loss\n",
    "        targets_ce = targets.squeeze(1).long() if targets.dim() == 4 else targets.long()\n",
    "        ce = self.ce_loss(predictions, targets_ce)\n",
    "        \n",
    "        # Dice Loss\n",
    "        targets_dice = targets.float().squeeze(1) if targets.dim() == 4 else targets.float()\n",
    "        dice = self.dice_loss(predictions, targets_dice)\n",
    "        return self.weight_ce * ce + self.weight_dice * dice\n",
    "\n",
    "# --- Training Function ---\n",
    "def train_unet_model(model, train_loader, val_loader, num_epochs=100, learning_rate=1e-3):\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = CombinedLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Train')\n",
    "        \n",
    "        for images, masks in train_pbar:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_pbar:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "                val_pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_unet_model.pth')\n",
    "            print(f'==> Best model saved with validation loss: {val_loss:.4f}')\n",
    "            \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d5a57958-6055-4af5-bc58-9a1628d705b6",
    "_uuid": "567295ae-841d-4194-a957-ee522822dd4b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1.3: Start U-Net Training\n",
    "# ===================================================================\n",
    "print(\"Starting U-Net model training...\")\n",
    "trained_model, train_losses, val_losses = train_unet_model(\n",
    "    model_unet_train, train_loader, val_loader, num_epochs=100, learning_rate=1e-3\n",
    ")\n",
    "\n",
    "# Plot the training history\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('U-Net Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "486c7185-abbc-46af-b7e9-13c5ee6e301d",
    "_uuid": "a0af59b5-fa2b-47a6-8cd7-bdbda5b32322",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## Part 2: YOLOv8 Model Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35374210-fc80-4f7e-a82b-5159627762cb",
    "_uuid": "51ebc8a4-c78d-4ba4-9430-fd73a004fc71",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 2.1: Start YOLOv8 Training\n",
    "# ===================================================================\n",
    "model_yolo_train = YOLO('yolov8n.pt') # Start from a pre-trained model\n",
    "\n",
    "print(\"Starting YOLOv8 model training...\")\n",
    "results = model_yolo_train.train(\n",
    "    data='/kaggle/input/cell-counting-roboflow-segmentation-masks/Cell_Counting_dataset_from_roboflow/data.yaml',\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    name='cell_detection_yolo_model' # Directory name for saving results\n",
    ")\n",
    "print(\"YOLOv8 training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f6662285-a6cd-4deb-b1ce-eeff6e759820",
    "_uuid": "05116803-8e78-41e2-bfd2-6e076659da0b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "---\n",
    "## Part 3: Inference on New Images from URLs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "945ff926-4534-402b-a466-e307bf4130d3",
    "_uuid": "f92a4f30-3339-4970-8f63-d18778dbccfa",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 3.1: Load Trained Models\n",
    "# ===================================================================\n",
    "\n",
    "# --- Load U-Net Model ---\n",
    "model_unet_eval = smp.Unet(\n",
    "    encoder_name=\"resnet152\",\n",
    "    encoder_weights=None, # No need to reload imagenet weights\n",
    "    classes=2,\n",
    "    activation=None,\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model_unet_eval = nn.DataParallel(model_unet_eval)\n",
    "# Load the state_dict from the saved file\n",
    "model_unet_eval.load_state_dict(torch.load(\"best_unet_model.pth\", map_location=device))\n",
    "model_unet_eval.to(device)\n",
    "model_unet_eval.eval()\n",
    "print(\"Trained U-Net model loaded successfully for inference.\")\n",
    "\n",
    "# --- Load YOLO Model ---\n",
    "model_path_yolo = '/kaggle/working/runs/detect/cell_detection_yolo_model/weights/best.pt'\n",
    "model_yolo_eval = YOLO(model_path_yolo)\n",
    "print(\"Trained YOLOv8 model loaded successfully for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7346c7ad-59ac-45bf-8b6f-8dc508e7c4d3",
    "_uuid": "40561d79-b043-41fa-9a06-4fcff5a6d6e6",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 3.2: Define Helper and Pipeline Functions for Inference\n",
    "# ===================================================================\n",
    "def estimate_average_cell_diameter(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None: return None\n",
    "    _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(thresh, connectivity=8)\n",
    "    areas = [stats[i, cv2.CC_STAT_AREA] for i in range(1, num_labels) if 10 < stats[i, cv2.CC_STAT_AREA] < 50000]\n",
    "    if not areas: return None\n",
    "    return np.sqrt(np.mean(areas) / np.pi) * 2\n",
    "\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    enhanced_contrast = clahe.apply(gray)\n",
    "    return cv2.cvtColor(enhanced_contrast, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "def post_process_mask(mask, min_area=25):\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    opened_mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(opened_mask, connectivity=8)\n",
    "    cleaned_mask = np.zeros_like(opened_mask)\n",
    "    for i in range(1, num_labels):\n",
    "        if stats[i, cv2.CC_STAT_AREA] >= min_area:\n",
    "            cleaned_mask[labels == i] = 255\n",
    "    return cleaned_mask\n",
    "\n",
    "def count_objects(binary_mask):\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return len(contours)\n",
    "\n",
    "def predict_unet_mask(model, device, image_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor.to(device))\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        binary = probs[:, 1, :, :] > 0.5\n",
    "        return binary.cpu().numpy().astype(np.uint8) * 255\n",
    "\n",
    "def unet_counting_pipeline(model, device, image_path, transform, min_area=25):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None: return 0, None, None\n",
    "    TARGET_CELL_DIAMETER = 60.0\n",
    "    current_diameter = estimate_average_cell_diameter(image_path)\n",
    "    if current_diameter is not None and current_diameter > 0:\n",
    "        resize_factor = TARGET_CELL_DIAMETER / current_diameter\n",
    "        new_size = (int(image.shape[1] * resize_factor), int(image.shape[0] * resize_factor))\n",
    "        image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    preprocessed_rgb = preprocess_image(image)\n",
    "    input_tensor = transform(image=preprocessed_rgb)[\"image\"].unsqueeze(0)\n",
    "    \n",
    "    pred_mask_batch = predict_unet_mask(model, device, input_tensor)\n",
    "    mask_resized = cv2.resize(pred_mask_batch[0], (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n",
    "    cleaned_mask = post_process_mask(mask_resized, min_area=min_area)\n",
    "    num_objects = count_objects(cleaned_mask)\n",
    "    return num_objects, cleaned_mask, image\n",
    "\n",
    "def count_cells_yolo(model, image_path, conf_threshold=0.45):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None: return 0, None, None\n",
    "    results = model(img, verbose=False, conf=conf_threshold)\n",
    "    return len(results[0].boxes), img, results\n",
    "\n",
    "def visualize_unet_prediction(original_bgr, cleaned_mask, num_objects):\n",
    "    contours, _ = cv2.findContours(cleaned_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    overlay = cv2.drawContours(original_bgr.copy(), contours, -1, (0, 255, 0), 2) # Green contours\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1); plt.imshow(cv2.cvtColor(original_bgr, cv2.COLOR_BGR2RGB)); plt.title('Original Image'); plt.axis('off')\n",
    "    plt.subplot(1, 2, 2); plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)); plt.title(f'U-Net Count: {num_objects}'); plt.axis('off');\n",
    "    plt.show()\n",
    "\n",
    "def visualize_yolo_prediction(original_bgr, yolo_results, num_objects):\n",
    "    overlay = original_bgr.copy()\n",
    "    for box in yolo_results[0].boxes.xyxy.cpu().numpy():\n",
    "        x1, y1, x2, y2 = map(int, box[:4])\n",
    "        cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2) # Green boxes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1); plt.imshow(cv2.cvtColor(original_bgr, cv2.COLOR_BGR2RGB)); plt.title('Original Image'); plt.axis('off')\n",
    "    plt.subplot(1, 2, 2); plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)); plt.title(f'YOLOv8 Count: {num_objects}'); plt.axis('off');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a5e1e46e-2d45-4aea-bd63-7c220d65116c",
    "_uuid": "bb087251-1852-4612-a2ab-ead02e1d1e4c",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 3.3: Run Inference on a List of Local Image Paths\n",
    "# ===================================================================\n",
    "image_paths = [\n",
    "    \"/kaggle/input/cell-counting-roboflow-segmentation-masks/test/images/Screenshot-2024-08-20-at-3-40-57-PM_png.rf.51490a3f822ef799797a83f5462ccc9a.jpg\",\n",
    "    \"/kaggle/input/cell-counting-roboflow-segmentation-masks/test/images/Screenshot-2024-08-20-at-3-41-22-PM_png.rf.44ac6ca63f784c0490e88c39d69692ed.jpg\",\n",
    "    \"/kaggle/input/cell-counting-roboflow-segmentation-masks/test/images/Screenshot-2024-08-20-at-6-10-00-PM_png.rf.988e622803e078ad613cd30a92b68a20.jpg\",\n",
    "    \"/kaggle/input/cell-counting-roboflow-segmentation-masks/test/images/Screenshot-2024-08-20-at-6-10-27-PM_png.rf.cf3e3aac50ec0abae4b45cb33971be50.jpg\"\n",
    "]\n",
    "\n",
    "UNET_MIN_CELL_AREA = 20\n",
    "YOLO_CONF_THRESHOLD = 0.5\n",
    "\n",
    "for i, path in enumerate(image_paths):\n",
    "    print(f\"\\n{'='*20} PROCESSING IMAGE {i+1} {'='*20}\")\n",
    "    print(f\"Path: {path}\")\n",
    "    \n",
    "    # Check if the file exists before processing\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Error: Image not found at path: {path}\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # --- Run U-Net Pipeline ---\n",
    "        pred_unet, final_mask, unet_img = unet_counting_pipeline(\n",
    "            model_unet_eval, device, path, val_test_transform, min_area=UNET_MIN_CELL_AREA\n",
    "        )\n",
    "        print(f\"U-Net Predicted Count: {pred_unet}\")\n",
    "        if unet_img is not None:\n",
    "            visualize_unet_prediction(unet_img, final_mask, pred_unet)\n",
    "            \n",
    "        # --- Run YOLOv8 Pipeline ---\n",
    "        pred_yolo, yolo_img, yolo_results = count_cells_yolo(\n",
    "            model_yolo_eval, path, conf_threshold=YOLO_CONF_THRESHOLD\n",
    "        )\n",
    "        print(f\"YOLOv8 Predicted Count: {pred_yolo}\")\n",
    "        if yolo_img is not None:\n",
    "            visualize_yolo_prediction(yolo_img, yolo_results, pred_yolo)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process image. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8050343,
     "sourceId": 12979025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
